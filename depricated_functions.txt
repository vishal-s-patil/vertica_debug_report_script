def prepare_date_conditions(from_date_time, to_date_time, query):
    """
    Prepares a list of tuples representing date conditions for a given query.
    
    :param date_time: The input date-time as a string in ISO 8601 format (e.g., '2025-01-15T00:00:00').
    :param table_datefield_columns_map: A dictionary mapping table names to date field column names.
    :param query: The SQL query string to extract the table name from.
    :return: A list of tuples containing the date conditions.
    """
    table_datefield_columns_map = {
        "sessions": "statement_start",
        "netstats.sessions_full": "statement_start",
        "netstats.resource_queues_full": "queue_entry_timestamp",
        "netstats.resource_queues": "queue_entry_time",
        "resource_queues": "queue_entry_timestamp",
        "netstats.storage_containers": "created_time",
        "netstats.error_messages": "event_timestamp",
        "netstats.query_profiles": "query_start",
        "query_profiles": "query_start"
    }
    try:
        # Parse the date_time string and calculate to_date_time (+1 day)
        # parsed_date_time = datetime.fromisoformat(date_time)
        parsed_from_date_time = datetime.strptime(from_date_time, '%Y-%m-%d %H:%M:%S')
        parsed_to_date_time = datetime.strptime(to_date_time, '%Y-%m-%d %H:%M:%S')
        # to_date_time = (parsed_date_time + timedelta(days=1)).isoformat()
        # to_date_time = (parsed_date_time + timedelta(days=1)).strftime('%Y-%m-%d %H:%M:%S')
        
        # Extract the table name from the query
        table_name = extract_table_name_from_query(query)
        if table_name not in table_datefield_columns_map:
            raise ValueError(f"Table name '{table_name}' not found in table_datefield_columns_map.")

        # Get the corresponding field from the mapping
        field = table_datefield_columns_map[table_name]

        # Prepare the conditions
        conditions = [
            (field, '>=', parsed_from_date_time, 'string'),
            (field, '<', parsed_to_date_time, 'string')
        ]
        return conditions
    except Exception as e:
        print(f"Error while preparing date conditions: {e}")
        return []

def add_where_conditions_to_query(query, where_conditions):
    """
    Adds where conditions to a query by replacing the <where_conditions> placeholder.
    Always adds an 'AND' before the conditions. Handles different types of values and conditions.

    :param query: SQL query string with a placeholder <where_conditions>.
    :param where_conditions: List of tuples containing:
                             (column_name, condition, value, type_of_value)
                             Example: [("age", ">", 18, "int"), ("status", "=", "active", "string")]
    :return: Modified query with the <where_conditions> placeholder replaced.
    """
    try:
        # Validate that the placeholder exists in the query
        if "<where_conditions>" not in query:
            raise ValueError("Query must contain the placeholder <where_conditions>.")

        # Build the WHERE clause
        where_clauses = []
        for column_name, condition, value, value_type in where_conditions:
            if value_type == "string":
                value = f"'{value}'"  # Add single quotes for string values
            elif value_type == "int":
                value = str(value)  # Keep integers as-is but convert to string for query
            
            where_clauses.append(f"{column_name} {condition} {value}")

        # Join conditions with 'AND' and prepend 'AND'
        final_conditions = " AND " + " AND ".join(where_clauses) if where_clauses else ""

        # Replace placeholder in the query
        return query.replace("<where_conditions>", final_conditions)

    except Exception as e:
        print(f"Error while adding where conditions: {e}")
        return query  


def extract_table_name_from_query(query):
    """
    Extracts the table name from the query based on predefined mappings.

    :param query: The SQL query string.
    :return: The table name as a string, or 'Unknown' if no match is found.
    """
    # Lowercase the query for case-insensitive matching
    query = query.lower()

    # Define the mappings of substrings to table names
    table_mappings = {
        "from resource_queues": "resource_queues",
        "from sessions": "sessions",
        "from netstats.error_messages": "netstats.error_messages",
        "from netstats.query_profiles": "netstats.query_profiles",
        "from error_messages": "error_messages",
        "from netstats.storage_containers": "netstats.storage_containers",
        "from netstats.sessions_full": "netstats.sessions_full",
        "from netstats.resource_queues_full": "netstats.resource_queues_full",
        "from query_profiles": "query_profiles"
    }

    # Check for each mapping in the query
    for substring, table_name in table_mappings.items():
        if substring in query:
            return table_name

    # Default return if no match is found
    return "Unknown"


def execute_queries_from_csv(csv_file_path, filters, verbose, is_now, queries_to_execute=None):
    try:
        vertica_connection = get_vertica_connection()
        if not vertica_connection:
            print("Failed to connect to the Vertica database. Exiting.")
            return

        with open(csv_file_path, mode='r') as file:
            csv_reader = csv.DictReader(file, delimiter='~')
            for row in csv_reader:
                qid = int(row['qid'])
                query_name = row['query_name']
                query = row['query']
                query_description = row['query_description']

                if is_now and query_name[-5:] == "_past":
                    continue

                is_past_query_present = False
                if not is_now:
                    is_past_query_present = check_if_past_query_present(query_name, csv_file_path)
                    if is_past_query_present:
                        query_name = query_name + '_past'
                
                if queries_to_execute and query_name not in queries_to_execute:
                    continue
                
                replaced_tables = False
                d = {}
                if filters['from_date_time'] is not None and is_past_query_present:
                    query = replace_tables_in_query(query)
                    replaced_tables = True
                if filters['to_date_time'] is not None and is_past_query_present:
                    if not replaced_tables:
                        replaced_tables = True
                        query = replace_tables_in_query(query)
                if filters['issue_time'] is not None and is_past_query_present:
                    if not is_now and not replaced_tables:
                        replaced_tables = True
                        query = replace_tables_in_query(query)
                
                for key, val in filters.items():
                    if val is not None:
                        d[key] = val

                query = replace_conditions(query, d)

                query = query.replace("<subcluster_name>", filters['subcluster_name'])
                
                if verbose:
                    print('QUERY: ', f"{query}", end="\n\n")
                
                query_result = execute_vertica_query(vertica_connection, query)
                if query_result == -1:
                    print(query_name, ": column not found\n")
                    continue
                query_result = process_query_result_and_highlight_text(query_result)

                if query_result:
                    
                    if query_name[-5:] == "_past":
                        query_name = query_name[:-5]
                    print(f"\n\nQuery Name: {query_name}")
                    print("-" * len(f"Query Name: {query_name}"))
                    print(f"Query Description: {query_description}")
                    print("-" * len(f"Query Description: {query_description}"))
                    column_headers = [desc[0] for desc in vertica_connection.cursor().description]
                    print(tabulate(query_result, headers=column_headers, tablefmt='grid'))
                else:
                    pass
                    #print("No data returned")
        
        vertica_connection.close()
    except Exception as e:
        print(f"Error while processing the CSV file or executing queries: {e}")


def check_if_past_query_present(query_name, csv_file_path):
    with open(csv_file_path, mode='r') as file:
        csv_reader = csv.DictReader(file, delimiter='~')
        count = 0
        for row in csv_reader:
            query_name_new = row['query_name']
            if query_name_new[-5:] == "_past":
                query_name_new = query_name_new[:-5]

            if query_name_new == query_name:
                count += 1
        
        return count == 2

{
    "qid": 1,
    "query_name": "deleted_row_count",
    "query_description": "description",
    "query": "select created_time, schema_name, projection_name, sum(total_row_cnt) row_count, sum(deleted_row_cnt) deleted_rows, sum(delete_vector_cnt) dv_count, n.subcluster_name from netstats.storage_containers as sc JOIN nodes AS n ON n.node_name = sc.node_name where 1=1 {projection_name ILIKE 'table_name'} and created_time=(select max(created_time) from netstats.storage_containers) group by created_time, schema_name,projection_name,n.subcluster_name order by deleted_rows desc limit {num_items};"
},